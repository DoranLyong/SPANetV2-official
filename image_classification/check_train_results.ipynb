{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log = pd.read_csv('./output/train/spanetv2_s18_hybrid-224/summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = training_log['eval_top1'].max()\n",
    "print(f\"{max:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log['eval_top1'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log.loc[training_log['eval_top1'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = training_log[training_log['eval_top1']== training_log['eval_top1'].max()]\n",
    "summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Determine Training Stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training and validation loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_log['epoch'], training_log['train_loss'], label='Training Loss')\n",
    "plt.plot(training_log['epoch'], training_log['eval_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "\n",
    "# Validation accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_log['epoch'], training_log['eval_top1'], label='Top-1 Accuracy')\n",
    "plt.plot(training_log['epoch'], training_log['eval_top5'], label='Top-5 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot learning rate progression\n",
    "#plt.figure(figsize=(6, 5))\n",
    "#plt.plot(training_log['epoch'], training_log['lr'])\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('Learning Rate')\n",
    "#plt.title('Learning Rate Progression')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Plots\n",
    "**Loss Plots**: \n",
    "* Both training and validation loss should decrease smoothly over epochs. Significant fluctuations or increases may indicate instability.\n",
    "\n",
    "**Accuracy Plots**: \n",
    "* Accuracy should improve steadily. Erratic behavior or lack of improvement might signal issues.\n",
    "\n",
    "**Learning Rate Plot**: \n",
    "* Ensure the learning rate increases smoothly during the warmup period and adjusts appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loss\n",
    "\n",
    "**Stable Training**: \n",
    "* Both training and validation loss decrease smoothly over epochs.\n",
    "\n",
    "**Unstable Training**: \n",
    "* If you see significant fluctuations or increases in the validation loss, it could indicate instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy\n",
    "\n",
    "**Stable Training**: \n",
    "* The top-1 and top-5 accuracy should improve steadily.\n",
    "\n",
    "**Unstable Training**: \n",
    "* Erratic behavior or lack of improvement in accuracy might indicate issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Progression\n",
    "**Stable Training**: \n",
    "* The learning rate increases smoothly during the warmup period and adjusts appropriately afterward.\n",
    "\n",
    "**Unstable Training**: \n",
    "* Abrupt changes or failure to adjust correctly can indicate instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# Actions to Take "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create example data to simulate erratic training behavior\n",
    "epochs = training_log['epoch']\n",
    "train_loss_erratic = np.sin(epochs / 2) * 0.5 + training_log['train_loss']  # Adding sinusoidal noise\n",
    "eval_loss_erratic = np.cos(epochs / 3) * 0.5 + training_log['eval_loss']  # Adding cosine noise\n",
    "eval_top1_erratic = np.clip(training_log['eval_top1'] + np.random.normal(0, 5, size=len(epochs)), 0, 100)  # Adding random noise\n",
    "eval_top5_erratic = np.clip(training_log['eval_top5'] + np.random.normal(0, 5, size=len(epochs)), 0, 100)  # Adding random noise\n",
    "\n",
    "# Plotting the erratic training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training and validation loss plot (erratic)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss_erratic, label='Training Loss (Erratic)')\n",
    "plt.plot(epochs, eval_loss_erratic, label='Validation Loss (Erratic)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Erratic Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Validation accuracy plot (erratic)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, eval_top1_erratic, label='Top-1 Accuracy (Erratic)')\n",
    "plt.plot(epochs, eval_top5_erratic, label='Top-5 Accuracy (Erratic)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Erratic Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe such erratic behavior in your training logs:\n",
    "\n",
    "**Learning Rate**: \n",
    "* Consider reducing the learning rate.\n",
    "\n",
    "**Warmup Period**: \n",
    "* Increase the number of warmup epochs to allow the model to stabilize before reaching the target learning rate.\n",
    "\n",
    "**Gradient Clipping**: \n",
    "* Implement gradient clipping to avoid exploding gradients.\n",
    "\n",
    "**Batch Size**: \n",
    "* Re-evaluate the effective batch size and adjust if necessary.\n",
    "\n",
    "By addressing these factors, you can improve the stability of your training process and achieve more consistent results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba4ab50ebab8b52f9c9e5ac369b73944c3ecb6b6f79fe994e1192d9da1adfecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
